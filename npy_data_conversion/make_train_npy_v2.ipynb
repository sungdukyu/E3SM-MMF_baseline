{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92ea9cbd-5853-4085-a168-50941f55988f",
   "metadata": {},
   "source": [
    "This script is to load original netcdf datasets / to preprocess / and to save them as npy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fbf8625-98ea-44ee-95bd-3a1095c392cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 22:16:15.970525: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import callbacks\n",
    "import keras_tuner as kt\n",
    "from keras_tuner import HyperModel\n",
    "from keras_tuner import RandomSearch\n",
    "import os\n",
    "import tensorflow_addons as tfa\n",
    "import sys\n",
    "import argparse\n",
    "import glob\n",
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aa4bf65-a3b7-4c76-bc9d-f0aa6453b9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2 vars\n",
    "# using all variables except:\n",
    "# 1. state_pmid is not used since it is a function of state_ps (e.g., hyam*p0+hybm*ps)\n",
    "# 2. Ozone and GHG variables in vars_mli_utls are used for levels [5,20],\n",
    "#.   where the variance is the largest.\n",
    "\n",
    "vars_mli      = ['state_t','state_q0001', 'state_q0002', 'state_q0003', 'state_u', 'state_v',\n",
    "                 'state_ps', 'pbuf_SOLIN','pbuf_LHFLX', 'pbuf_SHFLX',  'pbuf_TAUX', 'pbuf_TAUY', 'pbuf_COSZRS',\n",
    "                 'cam_in_ALDIF', 'cam_in_ALDIR', 'cam_in_ASDIF', 'cam_in_ASDIR', 'cam_in_LWUP',\n",
    "                 'cam_in_ICEFRAC', 'cam_in_LANDFRAC', 'cam_in_OCNFRAC', 'cam_in_SNOWHICE', 'cam_in_SNOWHLAND']\n",
    "vars_mli_utls = ['pbuf_ozone', 'pbuf_CH4', 'pbuf_N2O']\n",
    "vars_mlo      = ['ptend_t','ptend_q0001','ptend_q0002','ptend_q0003', 'ptend_u', 'ptend_v',\n",
    "                 'cam_out_NETSW', 'cam_out_FLWDS', 'cam_out_PRECSC', 'cam_out_PRECC',\n",
    "                 'cam_out_SOLS', 'cam_out_SOLL', 'cam_out_SOLSD', 'cam_out_SOLLD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e380525-916b-4ff7-a7b2-1d6b01ea8a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization/scaling factors\n",
    "# https://github.com/sungdukyu/E3SM-MMF_baseline/tree/main/norm_factors\n",
    "mli_mean  = xr.open_dataset('../norm_factors/mli_mean.nc',  engine='netcdf4')\n",
    "mli_min   = xr.open_dataset('../norm_factors/mli_min.nc',   engine='netcdf4')\n",
    "mli_max   = xr.open_dataset('../norm_factors/mli_max.nc',   engine='netcdf4')\n",
    "mlo_scale = xr.open_dataset('../norm_factors/mlo_scale.nc', engine='netcdf4')\n",
    "\n",
    "# for vars_mli_utls variables:\n",
    "# creating a clipped lev dimension called 'lev2'\n",
    "for k, kds in enumerate([mli_mean, mli_min, mli_max]):\n",
    "    kds_utls = kds[vars_mli_utls]\\\n",
    "          .isel(lev=slice(5,21)).rename({'lev':'lev2'})\n",
    "    kds = kds[vars_mli]\n",
    "    kds = kds.merge(kds_utls)\n",
    "    if k==0: mli_mean=kds\n",
    "    if k==1: mli_min=kds\n",
    "    if k==2: mli_max=kds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89e2f893-28a0-4cd6-90df-9fe11f1ed546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator for v2\n",
    "# (also includes data preprocessing)\n",
    "\n",
    "input_length = 425\n",
    "output_length = 368\n",
    "\n",
    "def load_nc_dir_with_generator(filelist:list):\n",
    "    def gen():\n",
    "        for file in filelist:\n",
    "\n",
    "            # input read / preprocess #\n",
    "            # read mli (-> ds)\n",
    "            ds = xr.open_dataset(file, engine='netcdf4')\n",
    "            # subset ozone, ch4, n2o\n",
    "            ds_utls = ds[vars_mli_utls]\\\n",
    "                      .isel(lev=slice(5,21)).rename({'lev':'lev2'})\n",
    "            # combine ds and ds_utls\n",
    "            ds = ds[vars_mli]\n",
    "            ds = ds.merge(ds_utls)\n",
    "\n",
    "            # output read / preprocess #\n",
    "            # read mlo (-> dso)\n",
    "            dso = xr.open_dataset(file.replace('.mli.','.mlo.'), engine='netcdf4')\n",
    "            # make mlo tendency variales (\"ptend_xxxx\"):\n",
    "            for kvar in ['state_t','state_q0001','state_q0002', 'state_q0003', 'state_u', 'state_v']:\n",
    "                dso[kvar.replace('state','ptend')] = (dso[kvar] - ds[kvar])/1200 # timestep=1200[sec]\n",
    "            # remove \"state_xxxx\"\n",
    "            dso = dso[vars_mlo]\n",
    "\n",
    "            # normalizatoin, scaling #\n",
    "            ds = (ds-mli_mean)/(mli_max-mli_min)\n",
    "            dso = dso*mlo_scale\n",
    "\n",
    "            # flatten input variables #\n",
    "            #ds = ds.stack({'batch':{'sample','ncol'}})\n",
    "            ds = ds.stack({'batch':{'ncol'}})\n",
    "            ds = ds.to_stacked_array(\"mlvar\", sample_dims=[\"batch\"], name='mli')\n",
    "            #dso = dso.stack({'batch':{'sample','ncol'}})\n",
    "            dso = dso.stack({'batch':{'ncol'}})\n",
    "            dso = dso.to_stacked_array(\"mlvar\", sample_dims=[\"batch\"], name='mlo')\n",
    "\n",
    "            yield (ds.values, dso.values)\n",
    "\n",
    "    return tf.data.Dataset.from_generator(gen,\n",
    "                                          output_types=(tf.float64, tf.float64),\n",
    "                                          output_shapes=((None,input_length),(None,output_length)),\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa4c7ebf-cd12-4a95-aeb3-a76e6af7f7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 22:16:27.870596: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-06-02 22:16:27.870660: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (nid005763): /proc/driver/nvidia/version does not exist\n",
      "2023-06-02 22:16:27.877665: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# save train -> npy\n",
    "\n",
    "# set stride\n",
    "stride_sample = 7 # prime number to sample all 'tod'\n",
    "\n",
    "# files (train)\n",
    "f_mli1 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.000[1234567]-*-*-*.nc')\n",
    "f_mli2 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.0008-01-*-*.nc')\n",
    "f_mli = sorted([*f_mli1, *f_mli2])\n",
    "# random.shuffle(f_mli)\n",
    "f_mli = f_mli[::stride_sample]\n",
    "\n",
    "# data generator -> npy array\n",
    "tds = load_nc_dir_with_generator(f_mli)\n",
    "work = list(tds.as_numpy_iterator())\n",
    "x_true = np.concatenate([ work[k][0] for k in range(len(work)) ])\n",
    "y_true = np.concatenate([ work[k][1] for k in range(len(work)) ])\n",
    "\n",
    "# to .npy\n",
    "with open(f'./npy_files/train_input.v2.stride-{stride_sample}.npy', 'wb') as f:\n",
    "    np.save(f, np.float32(x_true))\n",
    "with open(f'./npy_files/train_target.v2.stride-{stride_sample}.npy', 'wb') as f:\n",
    "    np.save(f, np.float32(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbfd1a38-72be-4677-8655-41007790c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save validation -> npy\n",
    "\n",
    "# set stride\n",
    "stride_sample = 7 # prime number to sample all 'tod'\n",
    "\n",
    "# files (val)\n",
    "f_mli1 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.0008-0[23456789]-*-*.nc')\n",
    "f_mli2 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.0008-1[012]-*-*.nc')\n",
    "f_mli3 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.0009-01-*-*.nc')\n",
    "f_mli_val = sorted([*f_mli1, *f_mli2, *f_mli3])\n",
    "# random.shuffle(f_mli_val)\n",
    "f_mli_val = f_mli_val[::stride_sample]\n",
    "\n",
    "# data generator -> npy array\n",
    "\n",
    "\n",
    "tds = load_nc_dir_with_generator(f_mli_val)\n",
    "work = list(tds.as_numpy_iterator())\n",
    "x_true = np.concatenate([ work[k][0] for k in range(len(work)) ])\n",
    "y_true = np.concatenate([ work[k][1] for k in range(len(work)) ])\n",
    "\n",
    "# to .npy\n",
    "with open(f'./npy_files/val_input.v2.stride-{stride_sample}.npy', 'wb') as f:\n",
    "    np.save(f, np.float32(x_true))\n",
    "with open(f'./npy_files/val_target.v2.stride-{stride_sample}.npy', 'wb') as f:\n",
    "    np.save(f, np.float32(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2303b8e0-191a-4aa6-85ee-276a74312d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save validation -> npy\n",
    "\n",
    "# set stride\n",
    "stride_sample = 6 # prime number to sample all 'tod'\n",
    "\n",
    "# files (val)\n",
    "f_mli1 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.0008-0[23456789]-*-*.nc')\n",
    "f_mli2 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.0008-1[012]-*-*.nc')\n",
    "f_mli3 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.0009-01-*-*.nc')\n",
    "f_mli_val = sorted([*f_mli1, *f_mli2, *f_mli3])\n",
    "# random.shuffle(f_mli_val)\n",
    "f_mli_val = f_mli_val[::stride_sample]\n",
    "\n",
    "# data generator -> npy array\n",
    "\n",
    "\n",
    "tds = load_nc_dir_with_generator(f_mli_val)\n",
    "work = list(tds.as_numpy_iterator())\n",
    "x_true = np.concatenate([ work[k][0] for k in range(len(work)) ])\n",
    "y_true = np.concatenate([ work[k][1] for k in range(len(work)) ])\n",
    "\n",
    "# to .npy\n",
    "with open(f'./npy_files/val_input.v2.stride-{stride_sample}.npy', 'wb') as f:\n",
    "    np.save(f, np.float32(x_true))\n",
    "with open(f'./npy_files/val_target.v2.stride-{stride_sample}.npy', 'wb') as f:\n",
    "    np.save(f, np.float32(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5b2ceb-3426-4bfd-8680-1854da5c8be3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv-tf_conda",
   "language": "python",
   "name": "myenv-tf_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
