{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fbf8625-98ea-44ee-95bd-3a1095c392cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from matplotlib import cm, colors\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import json\n",
    "import pickle\n",
    "import csv\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, os\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71d9b9ed-458c-4235-b3de-90a2a2a96895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in/out variable lists\n",
    "vars_mli = ['state_t','state_q0001','state_ps','pbuf_SOLIN', 'pbuf_LHFLX', 'pbuf_SHFLX']\n",
    "vars_mlo = ['ptend_t','ptend_q0001','cam_out_NETSW','cam_out_FLWDS','cam_out_PRECSC','cam_out_PRECC','cam_out_SOLS','cam_out_SOLL','cam_out_SOLSD','cam_out_SOLLD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aced78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ocean/projects/atm200007p/jlin96/neurips_proj/e3sm_train_npy\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77aca7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these for your HPC and repo path\n",
    "data_path = '/ocean/projects/atm200007p/walrus/for_jerry/train/'\n",
    "norm_path = '/ocean/projects/atm200007p/jlin96/neurips_proj/mooers_metrics/norm_factors/'\n",
    "grid_path = '/ocean/projects/atm200007p/jlin96/neurips_proj/mooers_metrics/test_data/E3SM-MMF_ne4_grid-info.orig.nc'\n",
    "save_path = '/ocean/projects/atm200007p/jlin96/neurips_proj/e3sm_train_npy/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4418fb23-4499-4d45-bbd2-8b63b21d91c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new dataset generator function\n",
    "# that has new options (latlim, lonlim)\n",
    "\n",
    "mli_mean = xr.open_dataset(norm_path + 'mli_mean.nc')\n",
    "mli_min = xr.open_dataset(norm_path + 'mli_min.nc')\n",
    "mli_max = xr.open_dataset(norm_path + 'mli_max.nc')\n",
    "mlo_scale = xr.open_dataset(norm_path + 'mlo_scale.nc')\n",
    "ne4_grid_info = xr.open_dataset(grid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9632043e-81c6-4b4d-93fa-20c608d3a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls(data_path = \"\"):\n",
    "    return os.popen(\" \".join([\"ls\", data_path])).read().splitlines()\n",
    "\n",
    "def concatenate_arrays(xrdata, vars):\n",
    "    return np.concatenate([np.atleast_1d(xrdata[var].values) for var in vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eee00472-b862-4551-949f-8b1a99a89e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mli_mean_npy = concatenate_arrays(mli_mean, vars_mli)\n",
    "mli_min_npy = concatenate_arrays(mli_min, vars_mli)\n",
    "mli_max_npy = concatenate_arrays(mli_max, vars_mli)\n",
    "mlo_scale_npy = concatenate_arrays(mlo_scale, vars_mlo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9de59a-0d4a-4774-a2cb-bcf26791fba6",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc89429e-01cb-42cb-a3ca-31e32a8353ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "latlim=[-999,999]\n",
    "lonlim=[-999,999]\n",
    "\n",
    "def showme_i(file):\n",
    "    ds = xr.open_dataset(file, engine = 'netcdf4')\n",
    "    ds = ds[vars_mli]\n",
    "    ds = ds.merge(ne4_grid_info[['lat','lon']])\n",
    "    ds = ds.where((ds['lat']>latlim[0])*(ds['lat']<latlim[1]),drop=True)\n",
    "    ds = ds.where((ds['lon']>lonlim[0])*(ds['lon']<lonlim[1]),drop=True)\n",
    "    return(ds)\n",
    "\n",
    "def showme_o(file):\n",
    "    # read mli\n",
    "    ds = xr.open_dataset(file, engine='netcdf4')\n",
    "    ds = ds[vars_mli]\n",
    "    ds = ds.merge(ne4_grid_info[['lat','lon']])\n",
    "    ds = ds.where((ds['lat']>latlim[0])*(ds['lat']<latlim[1]),drop=True)\n",
    "    ds = ds.where((ds['lon']>lonlim[0])*(ds['lon']<lonlim[1]),drop=True)\n",
    "\n",
    "    # read mlo\n",
    "    dso = xr.open_dataset(file.replace('.mli.','.mlo.'), engine='netcdf4')\n",
    "    dso = dso.merge(ne4_grid_info[['lat','lon']])\n",
    "    dso = dso.where((dso['lat']>latlim[0])*(dso['lat']<latlim[1]),drop=True)\n",
    "    dso = dso.where((dso['lon']>lonlim[0])*(dso['lon']<lonlim[1]),drop=True)\n",
    "\n",
    "    # make mlo variales: ptend_t and ptend_q0001\n",
    "    dso['ptend_t'] = (dso['state_t'] - ds['state_t'])/1200 # T tendency [K/s]\n",
    "    dso['ptend_q0001'] = (dso['state_q0001'] - ds['state_q0001'])/1200 # Q tendency [kg/kg/s]\n",
    "    dso = dso[vars_mlo]\n",
    "\n",
    "    # normalizatoin, scaling\n",
    "    # dso = dso*mlo_scale\n",
    "\n",
    "    # stack\n",
    "    #dso = dso.stack({'batch':{'sample','ncol'}})\n",
    "    # dso = dso.stack({'batch':{'ncol'}})\n",
    "    # dso = dso.to_stacked_array(\"mlvar\", sample_dims=[\"batch\"], name='mlo')\n",
    "    return dso\n",
    "\n",
    "def load_nc_dir_with_generator_test(filelist:list, latlim=[-999,999], lonlim=[-999,999]):\n",
    "    def gen():\n",
    "        for file in filelist:\n",
    "            \n",
    "            # read mli\n",
    "            ds = xr.open_dataset(file, engine='netcdf4')\n",
    "            ds = ds[vars_mli]\n",
    "            ds = ds.merge(ne4_grid_info[['lat','lon']])\n",
    "            ds = ds.where((ds['lat']>latlim[0])*(ds['lat']<latlim[1]),drop=True)\n",
    "            ds = ds.where((ds['lon']>lonlim[0])*(ds['lon']<lonlim[1]),drop=True)\n",
    "            \n",
    "            # read mlo\n",
    "            dso = xr.open_dataset(file.replace('.mli.','.mlo.'), engine='netcdf4')\n",
    "            dso = dso.merge(ne4_grid_info[['lat','lon']])\n",
    "            dso = dso.where((dso['lat']>latlim[0])*(dso['lat']<latlim[1]),drop=True)\n",
    "            dso = dso.where((dso['lon']>lonlim[0])*(dso['lon']<lonlim[1]),drop=True)\n",
    "            \n",
    "            # make mlo variales: ptend_t and ptend_q0001\n",
    "            dso['ptend_t'] = (dso['state_t'] - ds['state_t'])/1200 # T tendency [K/s]\n",
    "            dso['ptend_q0001'] = (dso['state_q0001'] - ds['state_q0001'])/1200 # Q tendency [kg/kg/s]\n",
    "            dso = dso[vars_mlo]\n",
    "            \n",
    "            # normalization, scaling\n",
    "            ds = (ds-mli_mean)/(mli_max-mli_min)\n",
    "            dso = dso*mlo_scale\n",
    "\n",
    "            # stack\n",
    "            #ds = ds.stack({'batch':{'sample','ncol'}})\n",
    "            ds = ds.stack({'batch':{'ncol'}})\n",
    "            ds = ds.to_stacked_array(\"mlvar\", sample_dims=[\"batch\"], name='mli')\n",
    "            #dso = dso.stack({'batch':{'sample','ncol'}})\n",
    "            dso = dso.stack({'batch':{'ncol'}})\n",
    "            dso = dso.to_stacked_array(\"mlvar\", sample_dims=[\"batch\"], name='mlo')\n",
    "            \n",
    "            yield (ds.values, dso.values)\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_types=(tf.float64, tf.float64),\n",
    "        output_shapes=((None,124),(None,128))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e595fab-cab1-44c8-9035-19d0f854401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path + 'train_input.npy', 'rb') as f:\n",
    "    train_input = np.load(f)\n",
    "\n",
    "with open(save_path + 'train_target.npy', 'rb') as f:\n",
    "    train_target = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17cd3f53-0f3d-4d90-aa19-b9bb369aaaed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10091520, 124)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7d9d886-08cf-43c5-9ab4-14beb28b1e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10091520, 128)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36cce09b-dfcd-4b83-8523-93ef7a6dd051",
   "metadata": {},
   "outputs": [],
   "source": [
    "latlons = {i: (ne4_grid_info[\"lat\"].values[i], ne4_grid_info[\"lon\"].values[i]) for i in range(384)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1d1c89c-b941-4c0b-a4a9-6935eb0b0c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path + 'indextolatlons.pkl', 'wb') as f:\n",
    "    pickle.dump(latlons, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae1a97e8-3921-4bf7-a067-e734f677d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_npy(var_arr):\n",
    "    var_arr = var_arr.reshape((int(var_arr.shape[0]/384), 384, 60))\n",
    "    return(var_arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
