{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37e66dba-4406-4d1d-9eeb-0168db894e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 15:14:33.156327: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-31 15:14:36.741209: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import csv\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, os\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e05061f3-69ad-41ed-9936-5ad32b34932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in/out variable lists\n",
    "vars_mli0 = ['state_t','state_q0001','state_q0002','state_q0003','state_ps']\n",
    "vars_mlo0 = ['state_t','state_q0001','state_q0002','state_q0003']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05d0338c-d2d4-451c-b088-5e701d58786e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#files: 26280\n"
     ]
    }
   ],
   "source": [
    "# validation dataset for HPO\n",
    "stride_sample = 5 \n",
    "f_mli1 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.0008-0[23456789]-*-*.nc')\n",
    "f_mli2 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.0008-1[012]-*-*.nc')\n",
    "f_mli3 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.0009-01-*-*.nc')\n",
    "f_mli_val = sorted([*f_mli1, *f_mli2, *f_mli3])\n",
    "f_mli = f_mli_val\n",
    "print(f'#files: {len(f_mli_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b1d78f1-2f1d-4dc6-9603-74dbe6fe4ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#files: 26280\n"
     ]
    }
   ],
   "source": [
    "# For testing <faster>\n",
    "stride_sample = 5 \n",
    "#f_mli1 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.0008-0[23456789]-*-*.nc')\n",
    "f_mli2 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.0008-1[012]-*-*.nc')\n",
    "#f_mli3 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.0009-01-*-*.nc')\n",
    "#f_mli_val = sorted([*f_mli1, *f_mli2, *f_mli3])\n",
    "f_mli = f_mli2\n",
    "print(f'#files: {len(f_mli_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "016a44a3-6783-4272-9f2f-fbdc29fe7707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new dataset generator function\n",
    "# that has new options (latlim, lonlim)\n",
    "mli_mean = xr.open_dataset('./norm_factors/mli_mean.nc')\n",
    "mli_min = xr.open_dataset('./norm_factors/mli_min.nc')\n",
    "mli_max = xr.open_dataset('./norm_factors/mli_max.nc')\n",
    "mlo_scale = xr.open_dataset('./norm_factors/mlo_scale.nc')\n",
    "ne4_grid_info = xr.open_dataset('./test_data/E3SM-MMF_ne4_grid-info.orig.nc')\n",
    "def load_nc_mli(filelist:list):\n",
    "    def gen():\n",
    "        for file in filelist:\n",
    "            \n",
    "            # read mli\n",
    "            ds = xr.open_dataset(file, engine='netcdf4')\n",
    "            ds = ds[vars_mli0]\n",
    "\n",
    "            # stack\n",
    "            #ds = ds.stack({'batch':{'sample','ncol'}})\n",
    "            ds = ds.stack({'batch':{'ncol'}})\n",
    "            ds = ds.to_stacked_array(\"mlvar\", sample_dims=[\"batch\"], name='mli')\n",
    "            #print(ds.shape)\n",
    "            #dso = dso.stack({'batch':{'sample','ncol'}})\n",
    "            \n",
    "            #print(ds.shape, dso.shape)\n",
    "            yield (ds.values)\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_types=(tf.float64),\n",
    "        output_shapes=((None,241))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d60ae70b-835d-4cfa-b903-0ba9818392aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new dataset generator function\n",
    "# that has new options (latlim, lonlim)\n",
    "mli_mean = xr.open_dataset('./norm_factors/mli_mean.nc')\n",
    "mli_min = xr.open_dataset('./norm_factors/mli_min.nc')\n",
    "mli_max = xr.open_dataset('./norm_factors/mli_max.nc')\n",
    "mlo_scale = xr.open_dataset('./norm_factors/mlo_scale.nc')\n",
    "ne4_grid_info = xr.open_dataset('./test_data/E3SM-MMF_ne4_grid-info.orig.nc')\n",
    "def load_nc_mlo(filelist:list):\n",
    "    def gen():\n",
    "        for file in filelist:\n",
    "            \n",
    "            # read mli\n",
    "            ds = xr.open_dataset(file.replace('.mli.','.mlo.'), engine='netcdf4')\n",
    "            ds = ds[vars_mli0]\n",
    "\n",
    "            # stack\n",
    "            #ds = ds.stack({'batch':{'sample','ncol'}})\n",
    "            ds = ds.stack({'batch':{'ncol'}})\n",
    "            ds = ds.to_stacked_array(\"mlvar\", sample_dims=[\"batch\"], name='mlo')\n",
    "            #print(ds.shape)\n",
    "            #dso = dso.stack({'batch':{'sample','ncol'}})\n",
    "            \n",
    "            #print(ds.shape, dso.shape)\n",
    "            yield (ds.values)\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_types=(tf.float64),\n",
    "        output_shapes=((None,240))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64752371-9523-4e1c-8598-8c7a88cd24ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 15:15:43.509495: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-05-31 15:15:43.682336: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "tds_testi  = load_nc_mli(f_mli)\n",
    "worki = list(tds_testi.as_numpy_iterator())\n",
    "x_true = np.concatenate([ worki[k] for k in range(len(worki)) ])\n",
    "tds_testo  = load_nc_mlo(f_mli)\n",
    "worko = list(tds_testi.as_numpy_iterator())\n",
    "y_true = np.concatenate([ worko[k] for k in range(len(worko)) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccd799b6-6845-4330-9a52-61fba26cf8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_in = x_true[:,:60]\n",
    "qv_in = x_true[:,60:120]\n",
    "ql_in = x_true[:,120:180]\n",
    "qs_in = x_true[:,180:240]\n",
    "ps_in = x_true[:,240]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7734934d-8375-4dea-addf-77edf3c43bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_out = y_true[:,:60]\n",
    "qv_out = y_true[:,60:120]\n",
    "ql_out = y_true[:,120:180]\n",
    "qs_out = y_true[:,180:240]\n",
    "ps_out = y_true[:,240]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "043eea5e-e029-42b2-b1c2-3ec9a15f12dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tv_in = T_in*(1 + 0.61*qv_in);\n",
    "Tv_out = T_out*(1 + 0.61*qv_out);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b72fde58-2365-469e-a87b-8b57189a78a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pressure\n",
    "def convert_to_Pressure(hyam,hybm,PS,P0):\n",
    "    Dimension1=hyam.shape\n",
    "    Dimension2=PS.shape\n",
    "    Pressure = np.zeros([Dimension2[0],Dimension1[0]])\n",
    "    \n",
    "    for i in range(Dimension2[0]):\n",
    "        #temp = (P0*hyam[:] + PS[i]*hybm[:])\n",
    "        #print(temp.shape)\n",
    "        Pressure[i,:] = (P0*hyam[:] + PS[i]*hybm[:])\n",
    "    return Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "535c5464-e741-49cf-aa4b-79b262b61625",
   "metadata": {},
   "outputs": [],
   "source": [
    "ne4_grid_info = xr.open_dataset('./test_data/E3SM-MMF_ne4_grid-info.orig.nc')\n",
    "lon = ne4_grid_info.lon.values\n",
    "lat = ne4_grid_info.lat.values\n",
    "area = ne4_grid_info.area.values\n",
    "hyam = ne4_grid_info.hyam.values\n",
    "hybm = ne4_grid_info.hybm.values\n",
    "PS = ne4_grid_info.PS.values\n",
    "P0 = ne4_grid_info.P0.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e80b0086-9cb7-44ba-85d0-503332ab4422",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pre_in = convert_to_Pressure(hyam,hybm,ps_in,P0)\n",
    "Pre_out = convert_to_Pressure(hyam,hybm,ps_out,P0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7174404a-781f-4f9d-804a-6b5163be60cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncol = 384\n",
    "nlev = 60\n",
    "tlen = 2543616"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4105f0c1-3ab5-416d-b9f5-4f7461dcd6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "constRdry  = 287              # J/K/kg\n",
    "cp_h2o_o   = 1869.4           #evaluated at tk_o and p_o\n",
    "cp_air_o   = 1005.2           #evaluated at tk_o and p_o\n",
    "cw_h2o_o   = 4218.0           #evaluated at tk_o\n",
    "ci_h2o_o   = 2106.0           #evaluated at tk_o\n",
    "latent_heat_vaporization = 2.501e6  # Latent heat of vaporization in J/kg\n",
    "rho_in     = Pre_in/(constRdry*Tv_in)\n",
    "rho_out     = Pre_out/(constRdry*Tv_out)\n",
    "cpmix      = cp_air_o + cp_h2o_o*qv_in + cw_h2o_o*ql_in+ci_h2o_o*qs_in;\n",
    "cpmixo      = cp_air_o + cp_h2o_o*qv_out + cw_h2o_o*ql_out+ci_h2o_o*qs_out;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c4d00af-fc47-4788-9b49-05fb21023435",
   "metadata": {},
   "outputs": [],
   "source": [
    "mweight = np.zeros([tlen,nlev])\n",
    "mweighto = np.zeros([tlen,nlev])\n",
    "for k in range(nlev):\n",
    "    mweight[:,k]    =  rho_in[:,k]/np.sum(rho_in,axis=1)\n",
    "    mweighto[:,k]    = rho_out[:,k]/np.sum(rho_out,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ef70822-9a50-4392-8a68-2b69b46ebe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new dataset generator function\n",
    "# that has new options (latlim, lonlim)\n",
    "\n",
    "mli_mean = xr.open_dataset('./norm_factors/mli_mean.nc')\n",
    "mli_min = xr.open_dataset('./norm_factors/mli_min.nc')\n",
    "mli_max = xr.open_dataset('./norm_factors/mli_max.nc')\n",
    "mlo_scale = xr.open_dataset('./norm_factors/mlo_scale.nc')\n",
    "ne4_grid_info = xr.open_dataset('./test_data/E3SM-MMF_ne4_grid-info.orig.nc')\n",
    "\n",
    "def load_nc_dir_with_generator_test(filelist:list, latlim=[-999,999], lonlim=[-999,999]):\n",
    "    def gen():\n",
    "        for file in filelist:\n",
    "            \n",
    "            # read mli\n",
    "            ds = xr.open_dataset(file, engine='netcdf4')\n",
    "            ds = ds[vars_mli]\n",
    "            ds = ds.merge(ne4_grid_info[['lat','lon']])\n",
    "            ds = ds.where((ds['lat']>latlim[0])*(ds['lat']<latlim[1]),drop=True)\n",
    "            ds = ds.where((ds['lon']>lonlim[0])*(ds['lon']<lonlim[1]),drop=True)\n",
    "            \n",
    "            # read mlo\n",
    "            dso = xr.open_dataset(file.replace('.mli.','.mlo.'), engine='netcdf4')\n",
    "            dso = dso.merge(ne4_grid_info[['lat','lon']])\n",
    "            dso = dso.where((dso['lat']>latlim[0])*(dso['lat']<latlim[1]),drop=True)\n",
    "            dso = dso.where((dso['lon']>lonlim[0])*(dso['lon']<lonlim[1]),drop=True)\n",
    "            \n",
    "            # make mlo variales: ptend_t and ptend_q0001\n",
    "            dso['ptend_t'] = (dso['state_t'] - ds['state_t'])/1200 # T tendency [K/s]\n",
    "            dso['ptend_q0001'] = (dso['state_q0001'] - ds['state_q0001'])/1200 # Q tendency [kg/kg/s]\n",
    "            dso = dso[vars_mlo]\n",
    "            \n",
    "            # normalizatoin, scaling\n",
    "            ds = (ds-mli_mean)/(mli_max-mli_min)\n",
    "            dso = dso*mlo_scale\n",
    "\n",
    "            # stack\n",
    "            #ds = ds.stack({'batch':{'sample','ncol'}})\n",
    "            ds = ds.stack({'batch':{'ncol'}})\n",
    "            ds = ds.to_stacked_array(\"mlvar\", sample_dims=[\"batch\"], name='mli')\n",
    "            #dso = dso.stack({'batch':{'sample','ncol'}})\n",
    "            dso = dso.stack({'batch':{'ncol'}})\n",
    "            dso = dso.to_stacked_array(\"mlvar\", sample_dims=[\"batch\"], name='mlo')\n",
    "            \n",
    "            yield (ds.values, dso.values)\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_types=(tf.float64, tf.float64),\n",
    "        output_shapes=((None,124),(None,128))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "768b5d16-79a7-4c63-91e2-e30df1f9cddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in/out variable lists\n",
    "vars_mli = ['state_t','state_q0001','state_ps','pbuf_SOLIN', 'pbuf_LHFLX', 'pbuf_SHFLX']\n",
    "vars_mlo = ['ptend_t','ptend_q0001','cam_out_NETSW','cam_out_FLWDS','cam_out_PRECSC','cam_out_PRECC','cam_out_SOLS','cam_out_SOLL','cam_out_SOLSD','cam_out_SOLLD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4deb0bff-5ff1-47b8-bbd9-fe662584d905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 15:21:36.782021: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "tds_test = load_nc_dir_with_generator_test(f_mli)\n",
    "work = list(tds_test.as_numpy_iterator())\n",
    "x_true_n = np.concatenate([ work[k][0] for k in range(len(work)) ])\n",
    "y_true_n = np.concatenate([ work[k][1] for k in range(len(work)) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20c56a4d-9a2a-4aa2-a405-8df0fe66187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function turns 20 minute increments into daily averages\n",
    "# (384)*ndays*72 = Dim[0]\n",
    "# Output dim: (384)*ndays\n",
    "# Griffin's original version is based on regrided data\n",
    "# This version is based on ne4pg2 grid (384 columns)\n",
    "# This change allow me to use Sungduk's code directly\n",
    "# since he generaged the min/max/scale already\n",
    "def average_data_Griffin_LiranEdited(reconstructed_targets,reconstructed_features):\n",
    "    Dim = reconstructed_targets.shape\n",
    "    x = 384\n",
    "    tnum = 72\n",
    "    t = Dim[0]\n",
    "    z = Dim[1]\n",
    "    ndays = int(t/(tnum*x))\n",
    "    \n",
    "    target_days = np.zeros(shape=(x*ndays,tnum, z))\n",
    "    feat_days = np.zeros(shape=(x*ndays,tnum, z))\n",
    "    day_array_targ = np.zeros(shape=(x,tnum,ndays, z))\n",
    "    day_array_feat = np.zeros(shape=(x,tnum,ndays, z))\n",
    "    #print(day_array_feat.shape)\n",
    "    ncol_count = 0\n",
    "    tstep_count = 0\n",
    "    day_count = 0\n",
    "    \n",
    "    for i in range(t):\n",
    "        temp_targ = reconstructed_targets[i, :]\n",
    "        day_array_targ[ncol_count,tstep_count,day_count, :] = temp_targ\n",
    "        temp_feat = reconstructed_features[i, :]\n",
    "        day_array_feat[ncol_count,tstep_count,day_count,:] = temp_feat\n",
    "        \n",
    "        if (ncol_count == x-1):\n",
    "            ncol_count = 0\n",
    "            tstep_count = tstep_count+1\n",
    "        else:\n",
    "            ncol_count = ncol_count+1\n",
    "        \n",
    "        if (tstep_count == tnum):\n",
    "            tstep_count = 0\n",
    "            day_count = day_count+1   \n",
    "            \n",
    "            \n",
    "    day_array_targ_out = np.nanmean(day_array_targ, axis = 1)\n",
    "    day_array_feat_out = np.nanmean(day_array_feat, axis = 1)\n",
    "    \n",
    "    return day_array_targ_out,day_array_feat_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fe46fd3-6bd7-41cc-be7f-c0bdba429b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import netCDF4 as nc\n",
    "folder_path = \"/pscratch/sd/h/heroplr/step2_retrain/backup_phase-4_retrained_models\"  # Replace with the actual folder path\n",
    "out_folder = \"/pscratch/sd/h/heroplr/R2_analysis_all/\"\n",
    "Dim_true = x_true.shape\n",
    "# Loop through all files in the folder\n",
    "numday = int(Dim_true[0]/ncol/72)\n",
    "filename=\"step2_lot-89_trial_0023.best.h5\"\n",
    "file_path = os.path.join(folder_path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23946bf2-1d1a-49f8-a825-a1f6846dec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(file_path,compile=False)\n",
    "y_pred = model(x_true_n)\n",
    "T_tend_true = y_true_n[:,:60]\n",
    "T_pred_true = y_pred[:,:60]\n",
    "Q_tend_true = y_true_n[:,60:120]\n",
    "Q_pred_true =y_pred[:,60:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee5eaa6e-7ae3-4138-b6ff-2f52b6c92ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_pre = T_in+T_pred_true*1200\n",
    "q_pre = qv_in+Q_pred_true*1200\n",
    "Tv_pre = T_pre*(1 + 0.61*q_pre);\n",
    "rho_pre     = Pre_in/(constRdry*Tv_pre)\n",
    "cpmixpre      = cp_air_o + cp_h2o_o*q_pre + cw_h2o_o*ql_in+ci_h2o_o*qs_in;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cb55fce-471f-4b18-8469-a6f1591084c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mweightpre = np.zeros([tlen,nlev])\n",
    "for k in range(nlev):\n",
    "    mweightpre[:,k]    =  rho_pre[:,k]/np.sum(rho_pre,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da3b9b8d-b483-4052-bd39-826ec0653d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideally, we should calculate mweight for pred also, but here I am using an estimated weight for the true. \n",
    "T_tend_true_mweight    = T_tend_true*mweighto*cpmixo\n",
    "T_pred_true_mweight    = T_pred_true*mweightpre*cpmixpre\n",
    "Q_tend_true_mweight    = Q_tend_true*mweighto*latent_heat_vaporization\n",
    "Q_pred_true_mweight    = Q_pred_true*mweightpre*latent_heat_vaporization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e604a490-2191-431e-8066-bb0e1ccb5b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_tend_true2,T_pred_true2 = average_data_Griffin_LiranEdited(T_tend_true_mweight,T_pred_true_mweight)\n",
    "Q_tend_true2,Q_pred_true2 = average_data_Griffin_LiranEdited(Q_tend_true_mweight,Q_pred_true_mweight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cec3822-f8ae-4d51-b063-28f2ed909326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new NetCDF file\n",
    "filename = file_path[-31:]+\"_mwv2.nc\"\n",
    "file_path_out = os.path.join(out_folder, filename)\n",
    "        \n",
    "ncfile = nc.Dataset(file_path_out, \"w\", format=\"NETCDF4\")\n",
    "\n",
    "# Define the dimensions\n",
    "time_dim = ncfile.createDimension(\"time\", None)  # Unlimited dimension\n",
    "lat_dim = ncfile.createDimension(\"ncol\", ncol)\n",
    "lon_dim = ncfile.createDimension(\"nlev\", nlev)\n",
    "day_dim = ncfile.createDimension(\"nday\", numday)\n",
    "# Create variables\n",
    "time_var = ncfile.createVariable(\"time\", \"f8\", (\"time\",))\n",
    "lon_var = ncfile.createVariable(\"lon\", \"f4\", (\"ncol\",))\n",
    "lat_var = ncfile.createVariable(\"lat\", \"f4\", (\"ncol\",))\n",
    "data_var3 = ncfile.createVariable(\"mwT_tend\", \"f8\", (\"nlev\",\"nday\",\"ncol\"))\n",
    "data_var4 = ncfile.createVariable(\"mwT_pred\", \"f8\", (\"nlev\",\"nday\",\"ncol\"))\n",
    "data_var5 = ncfile.createVariable(\"mwQ_tend\", \"f8\", (\"nlev\",\"nday\",\"ncol\"))\n",
    "data_var6 = ncfile.createVariable(\"mwQ_pred\", \"f8\", (\"nlev\",\"nday\",\"ncol\"))\n",
    "\n",
    "# Assign values to variables\n",
    "time_var[:] = [1]  # Example time values\n",
    "lon_var[:] = lon    # Example latitude values\n",
    "lat_var[:] = lat  # Example longitude values\n",
    "\n",
    "data_var3[:,:,:] = np.transpose(T_tend_true2)\n",
    "data_var4[:,:,:] = np.transpose(T_pred_true2)\n",
    "data_var5[:,:,:] = np.transpose(Q_tend_true2)\n",
    "data_var6[:,:,:] = np.transpose(Q_pred_true2)\n",
    "# Add global attributes\n",
    "ncfile.description = \"R2\"\n",
    "ncfile.history = \"Created by Liran\"\n",
    "\n",
    "# Close the NetCDF file\n",
    "ncfile.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbddc4d7-fe94-4b27-97f2-981e45690c15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bac3ea-1529-4242-a57f-008a664a3474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19607cec-25d7-4265-8a4b-abf74ac2197d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2d07ca-7dd5-4d05-bbdb-ba75966f521f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9c4d7f-8217-4844-b1ef-bc6237d5d0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
