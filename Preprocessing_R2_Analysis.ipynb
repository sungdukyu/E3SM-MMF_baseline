{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fbf8625-98ea-44ee-95bd-3a1095c392cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 21:54:23.550027: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-24 21:54:24.262275: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 21:54:25.125773: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from matplotlib import cm, colors\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import json\n",
    "import pickle\n",
    "import csv\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, os\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "try:\n",
    "    for kgpu in range(len(physical_devices)):\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[kgpu], True)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass\n",
    "#import psyplot.plotting.mapplot as mplot\n",
    "import psyplot.project as psy\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "%config InlineBackend.close_figures = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71d9b9ed-458c-4235-b3de-90a2a2a96895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in/out variable lists\n",
    "vars_mli = ['state_t','state_q0001','state_ps','pbuf_SOLIN', 'pbuf_LHFLX', 'pbuf_SHFLX']\n",
    "vars_mlo = ['ptend_t','ptend_q0001','cam_out_NETSW','cam_out_FLWDS','cam_out_PRECSC','cam_out_PRECC','cam_out_SOLS','cam_out_SOLL','cam_out_SOLSD','cam_out_SOLLD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6193ef32-667b-49a7-a820-23f1ea5535f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new dataset generator function\n",
    "# that has new options (latlim, lonlim)\n",
    "\n",
    "mli_mean = xr.open_dataset('./norm_factors/mli_mean.nc')\n",
    "mli_min = xr.open_dataset('./norm_factors/mli_min.nc')\n",
    "mli_max = xr.open_dataset('./norm_factors/mli_max.nc')\n",
    "mlo_scale = xr.open_dataset('./norm_factors/mlo_scale.nc')\n",
    "ne4_grid_info = xr.open_dataset('./test_data/E3SM-MMF_ne4_grid-info.orig.nc')\n",
    "\n",
    "def load_nc_dir_with_generator_test(filelist:list, latlim=[-999,999], lonlim=[-999,999]):\n",
    "    def gen():\n",
    "        for file in filelist:\n",
    "            \n",
    "            # read mli\n",
    "            ds = xr.open_dataset(file, engine='netcdf4')\n",
    "            ds = ds[vars_mli]\n",
    "            ds = ds.merge(ne4_grid_info[['lat','lon']])\n",
    "            ds = ds.where((ds['lat']>latlim[0])*(ds['lat']<latlim[1]),drop=True)\n",
    "            ds = ds.where((ds['lon']>lonlim[0])*(ds['lon']<lonlim[1]),drop=True)\n",
    "            \n",
    "            # read mlo\n",
    "            dso = xr.open_dataset(file.replace('.mli.','.mlo.'), engine='netcdf4')\n",
    "            dso = dso.merge(ne4_grid_info[['lat','lon']])\n",
    "            dso = dso.where((dso['lat']>latlim[0])*(dso['lat']<latlim[1]),drop=True)\n",
    "            dso = dso.where((dso['lon']>lonlim[0])*(dso['lon']<lonlim[1]),drop=True)\n",
    "            \n",
    "            # make mlo variales: ptend_t and ptend_q0001\n",
    "            dso['ptend_t'] = (dso['state_t'] - ds['state_t'])/1200 # T tendency [K/s]\n",
    "            dso['ptend_q0001'] = (dso['state_q0001'] - ds['state_q0001'])/1200 # Q tendency [kg/kg/s]\n",
    "            dso = dso[vars_mlo]\n",
    "            \n",
    "            # normalizatoin, scaling\n",
    "            ds = (ds-mli_mean)/(mli_max-mli_min)\n",
    "            dso = dso*mlo_scale\n",
    "\n",
    "            # stack\n",
    "            #ds = ds.stack({'batch':{'sample','ncol'}})\n",
    "            ds = ds.stack({'batch':{'ncol'}})\n",
    "            ds = ds.to_stacked_array(\"mlvar\", sample_dims=[\"batch\"], name='mli')\n",
    "            #dso = dso.stack({'batch':{'sample','ncol'}})\n",
    "            dso = dso.stack({'batch':{'ncol'}})\n",
    "            dso = dso.to_stacked_array(\"mlvar\", sample_dims=[\"batch\"], name='mlo')\n",
    "            \n",
    "            yield (ds.values, dso.values)\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_types=(tf.float64, tf.float64),\n",
    "        output_shapes=((None,124),(None,128))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ed882fa-56cb-4b25-b1da-7dc78dd71e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon = ne4_grid_info.lon.values\n",
    "lat = ne4_grid_info.lat.values\n",
    "area = ne4_grid_info.area.values\n",
    "hyam = ne4_grid_info.hyam.values\n",
    "hybm = ne4_grid_info.hybm.values\n",
    "PS = ne4_grid_info.PS.values\n",
    "P0 = ne4_grid_info.P0.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b764796-7eb5-433f-9e18-192631bdb298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pressure\n",
    "def convert_to_Pressure(hyam,hybm,PS,P0):\n",
    "    Dimension1=hyam.shape\n",
    "    Dimension2=PS.shape\n",
    "    Pressure = np.zeros([Dimension1[0],Dimension2[1]])\n",
    "    for i in range(Dimension2[1]):\n",
    "        #temp = (P0*hyam[:] + PS[i]*hybm[:])\n",
    "        #print(temp.shape)\n",
    "        Pressure[:,i] = (hyam[:] + PS[0,i]*hybm[:])\n",
    "    return Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51f4b3de-fa7f-4221-97ba-ac68dc082862",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pre = convert_to_Pressure(hyam,hybm,PS,P0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83de83a5-92d2-491f-8a99-1c7d90935662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate R2 based on Mooers et al. diagnostics\n",
    "def estimate_R2(y_true,y_pred,ncol,nlev):\n",
    "    Dimension=y_true.shape\n",
    "    R2 = np.zeros([ncol,nlev])\n",
    "    SSE = np.zeros([ncol,nlev])\n",
    "    SVAR = np.zeros([ncol,nlev])\n",
    "    for i in range(ncol):\n",
    "        for z in range(nlev):\n",
    "            y_true_temp = y_true[i::ncol,z]\n",
    "            y_pred_temp = y_pred[i::ncol,z]\n",
    "            y_true_temp.shape\n",
    "            SSE[i,z] = np.sum((y_true_temp-y_pred_temp)**2.0)\n",
    "            SVAR[i,z] = np.sum((y_true_temp-np.mean(y_true_temp))**2.0)\n",
    "            R2[i,z] = 1-(SSE[i,z]/SVAR[i,z])\n",
    "        \n",
    "    return SSE,SVAR,R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "599ba1e1-717f-46ea-b239-fdd95aac0cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate R2 based on Mooers et al. diagnostics\n",
    "def average_data(data,window_size):\n",
    "    \n",
    "    num_elements = data.shape[0]\n",
    "    num_windows = num_elements // window_size\n",
    "\n",
    "    # Reshape the data into windows of size 384 along the first dimension\n",
    "    data_windows = np.reshape(data[:num_windows * window_size], (num_windows, window_size, *data.shape[1:]))\n",
    "    #print(data_windows.shape)\n",
    "    # Calculate the average along the first dimension\n",
    "    averaged_data = np.mean(data_windows, axis=1)\n",
    "    return averaged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "277226b3-625e-4d40-a0fb-ddc9f2bc21ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function turns 20 minute increments into daily averages\n",
    "# (384)*ndays*72 = Dim[0]\n",
    "# Output dim: (384)*ndays\n",
    "# Griffin's original version is based on regrided data\n",
    "# This version is based on ne4pg2 grid (384 columns)\n",
    "# This change allow me to use Sungduk's code directly\n",
    "# since he generaged the min/max/scale already\n",
    "def average_data_Griffin_LiranEdited(reconstructed_targets,reconstructed_features):\n",
    "    Dim = reconstructed_targets.shape\n",
    "    x = 384\n",
    "    tnum = 72\n",
    "    t = Dim[0]\n",
    "    z = Dim[1]\n",
    "    ndays = int(t/(tnum*x))\n",
    "    \n",
    "    target_days = np.zeros(shape=(x*ndays,tnum, z))\n",
    "    feat_days = np.zeros(shape=(x*ndays,tnum, z))\n",
    "    day_array_targ = np.zeros(shape=(x,tnum,ndays, z))\n",
    "    day_array_feat = np.zeros(shape=(x,tnum,ndays, z))\n",
    "    #print(day_array_feat.shape)\n",
    "    ncol_count = 0\n",
    "    tstep_count = 0\n",
    "    day_count = 0\n",
    "    \n",
    "    for i in range(t):\n",
    "        temp_targ = reconstructed_targets[i, :]\n",
    "        day_array_targ[ncol_count,tstep_count,day_count, :] = temp_targ\n",
    "        temp_feat = reconstructed_features[i, :]\n",
    "        day_array_feat[ncol_count,tstep_count,day_count,:] = temp_feat\n",
    "        \n",
    "        if (ncol_count == x-1):\n",
    "            ncol_count = 0\n",
    "            tstep_count = tstep_count+1\n",
    "        else:\n",
    "            ncol_count = ncol_count+1\n",
    "        \n",
    "        if (tstep_count == tnum):\n",
    "            tstep_count = 0\n",
    "            day_count = day_count+1   \n",
    "            \n",
    "            \n",
    "    day_array_targ_out = np.nanmean(day_array_targ, axis = 1)\n",
    "    day_array_feat_out = np.nanmean(day_array_feat, axis = 1)\n",
    "    \n",
    "    return day_array_targ_out,day_array_feat_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50070607-f931-4b0b-bd05-eda88f225070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#files: 26280\n"
     ]
    }
   ],
   "source": [
    "# validation dataset for HPO\n",
    "stride_sample = 73 # about ~11% assuming we will use 1/5 subsampled dataset for full training.\n",
    "f_mli1 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.0008-0[23456789]-*-*.nc')\n",
    "f_mli2 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.0008-1[012]-*-*.nc')\n",
    "f_mli3 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.0009-01-*-*.nc')\n",
    "f_mli_val = sorted([*f_mli1, *f_mli2, *f_mli3])\n",
    "f_mli = f_mli_val[::stride_sample]\n",
    "print(f'#files: {len(f_mli_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e595fab-cab1-44c8-9035-19d0f854401f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 23:18:30.907857: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "# creating numpy array defeats the purpose of tf Dataset pipeline,\n",
    "# but, just doing it here for quick sanity check.\n",
    "tds_test = load_nc_dir_with_generator_test(f_mli)\n",
    "work = list(tds_test.as_numpy_iterator())\n",
    "x_true = np.concatenate([ work[k][0] for k in range(len(work)) ])\n",
    "y_true = np.concatenate([ work[k][1] for k in range(len(work)) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1be0058a-dd2c-4812-a537-d0da6f2f44cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncol = 384\n",
    "nlev = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ad23a62-07d0-435e-acce-cfc8774faff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-32_trial_0109.last.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98924/1243132100.py:14: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  R2[i,z] = 1-(SSE[i,z]/SVAR[i,z])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-29_trial_0098.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-25_trial_0003.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-31_trial_0089.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-60_trial_0027.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-43_trial_0081.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-41_trial_0023.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-41_trial_0103.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-38_trial_0091.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-29_trial_0144.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-31_trial_0089.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-42_trial_0110.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-45_trial_0049.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-32_trial_0105.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-50_trial_0070.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-50_trial_0036.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-41_trial_0023.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-57_trial_0023.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-29_trial_0098.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-63_trial_0024.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-41_trial_0128.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-61_trial_0024.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-45_trial_0095.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-45_trial_0049.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-27_trial_0109.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-37_trial_0035.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-31_trial_0030.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-50_trial_0105.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-32_trial_0105.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-27_trial_0086.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-45_trial_0048.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-60_trial_0027.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-39_trial_0081.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-61_trial_0024.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-39_trial_0099.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-29_trial_0144.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-41_trial_0100.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-42_trial_0023.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-38_trial_0046.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-42_trial_0062.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-39_trial_0099.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-37_trial_0031.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-37_trial_0031.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-31_trial_0030.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-35_trial_0102.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-45_trial_0095.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-27_trial_0086.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-33_trial_0093.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-25_trial_0003.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-36_trial_0023.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-60_trial_0022.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-50_trial_0036.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-33_trial_0142.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-40_trial_0111.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-33_trial_0142.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-41_trial_0128.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-41_trial_0100.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-41_trial_0103.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-39_trial_0081.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-37_trial_0135.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-43_trial_0081.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-57_trial_0023.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-42_trial_0023.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-29_trial_0022.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-63_trial_0024.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-45_trial_0048.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-33_trial_0093.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-35_trial_0070.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-35_trial_0070.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-37_trial_0135.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-35_trial_0102.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-60_trial_0022.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-27_trial_0109.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-65_trial_0023.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-36_trial_0023.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-65_trial_0023.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-42_trial_0110.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-38_trial_0091.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-38_trial_0083.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-50_trial_0105.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-50_trial_0070.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-29_trial_0022.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-37_trial_0035.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-38_trial_0046.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-42_trial_0062.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-40_trial_0111.best.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-38_trial_0083.last.h5\n",
      "/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models/step2_lot-32_trial_0109.best.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import netCDF4 as nc\n",
    "folder_path = \"/pscratch/sd/s/sungduk/for_neurips/backup_phase-3_retrained_models\"  # Replace with the actual folder path\n",
    "out_folder = \"/pscratch/sd/h/heroplr/R2_analysis/\"\n",
    "Dim_true = x_true.shape\n",
    "# Loop through all files in the folder\n",
    "numday = int(Dim_true[0]/ncol/72)\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    \n",
    "    if os.path.isfile(file_path):\n",
    "        # Perform operations on the file\n",
    "        print(file_path)  # Example: Print the file path\n",
    "        model = keras.models.load_model(file_path,compile=False)\n",
    "        y_pred = model(x_true)\n",
    "        T_tend_true = y_true[:,:60]\n",
    "        T_pred_true = y_pred[:,:60]\n",
    "        Q_tend_true = y_true[:,60:120]\n",
    "        Q_pred_true =y_pred[:,60:120]\n",
    "        T_tend_true_avg = average_data(T_tend_true,len(f_mli))\n",
    "        T_pred_true_avg = average_data(T_pred_true,len(f_mli))\n",
    "        Q_tend_true_avg = average_data(Q_tend_true,len(f_mli))\n",
    "        Q_pred_true_avg = average_data(Q_pred_true,len(f_mli))\n",
    "        T_tend_true2,T_pred_true2 = average_data_Griffin_LiranEdited(T_tend_true,T_pred_true)\n",
    "        Q_tend_true2,Q_pred_true2 = average_data_Griffin_LiranEdited(Q_tend_true,Q_pred_true)\n",
    "        T_tend_true2_reshaped_array = T_tend_true2.reshape((384*numday, 60))\n",
    "        T_pred_true2_reshaped_array = T_pred_true2.reshape((384*numday, 60))\n",
    "        Q_tend_true2_reshaped_array = Q_tend_true2.reshape((384*numday, 60))\n",
    "        Q_pred_true2_reshaped_array = Q_pred_true2.reshape((384*numday, 60))\n",
    "        TSSE,TSVAR,TR_temp = estimate_R2(T_tend_true2_reshaped_array,T_pred_true2_reshaped_array,384*numday,nlev)\n",
    "        QSSE,QSVAR,QR_temp = estimate_R2(Q_tend_true2_reshaped_array,Q_pred_true2_reshaped_array,384*numday,nlev)\n",
    "        TR1 = TR_temp.reshape((384,numday, 60))\n",
    "        QR1 = QR_temp.reshape((384,numday, 60))\n",
    "        TR2 =  np.nanmean(TR1, axis = 1)\n",
    "        QR2 =  np.nanmean(QR1, axis = 1)\n",
    "        # Create a new NetCDF file\n",
    "        filename = file_path[-31:]+\".nc\"\n",
    "        file_path_out = os.path.join(out_folder, filename)\n",
    "        \n",
    "        ncfile = nc.Dataset(file_path_out, \"w\", format=\"NETCDF4\")\n",
    "\n",
    "        # Define the dimensions\n",
    "        time_dim = ncfile.createDimension(\"time\", None)  # Unlimited dimension\n",
    "        lat_dim = ncfile.createDimension(\"ncol\", ncol)\n",
    "        lon_dim = ncfile.createDimension(\"nlev\", nlev)\n",
    "        day_dim = ncfile.createDimension(\"nday\", numday)\n",
    "        # Create variables\n",
    "        time_var = ncfile.createVariable(\"time\", \"f8\", (\"time\",))\n",
    "        lon_var = ncfile.createVariable(\"lon\", \"f4\", (\"ncol\",))\n",
    "        lat_var = ncfile.createVariable(\"lat\", \"f4\", (\"ncol\",))\n",
    "        PRE_var = ncfile.createVariable(\"P\", \"f8\", (\"nlev\",\"ncol\"))\n",
    "        data_var = ncfile.createVariable(\"TR2\", \"f8\", (\"nlev\",\"ncol\"))\n",
    "        data_var2 = ncfile.createVariable(\"QR2\", \"f8\", (\"nlev\",\"ncol\"))\n",
    "        data_var3 = ncfile.createVariable(\"T_tend_true_avg\", \"f8\", (\"nlev\",\"nday\",\"ncol\"))\n",
    "        data_var4 = ncfile.createVariable(\"T_pred_true_avg\", \"f8\", (\"nlev\",\"nday\",\"ncol\"))\n",
    "        data_var5 = ncfile.createVariable(\"Q_tend_true_avg\", \"f8\", (\"nlev\",\"nday\",\"ncol\"))\n",
    "        data_var6 = ncfile.createVariable(\"Q_pred_true_avg\", \"f8\", (\"nlev\",\"nday\",\"ncol\"))\n",
    "\n",
    "        # Assign values to variables\n",
    "        time_var[:] = [1]  # Example time values\n",
    "        lon_var[:] = lon    # Example latitude values\n",
    "        lat_var[:] = lat  # Example longitude values\n",
    "        PRE_var[:] = (Pre)\n",
    "        data_var[:,:] = np.transpose(TR2 )              # Example data values\n",
    "        data_var2[:,:] = np.transpose(QR2)\n",
    "\n",
    "        data_var3[:,:,:] = np.transpose(T_tend_true2)\n",
    "        data_var4[:,:,:] = np.transpose(T_pred_true2)\n",
    "        data_var5[:,:,:] = np.transpose(Q_tend_true2)\n",
    "        data_var6[:,:,:] = np.transpose(Q_pred_true2)\n",
    "        # Add global attributes\n",
    "        ncfile.description = \"R2\"\n",
    "        ncfile.history = \"Created by Liran\"\n",
    "\n",
    "        # Close the NetCDF file\n",
    "        ncfile.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c825295e-afe0-445a-9204-4381fe7bf119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9584df65-6c84-4b79-8e13-56ef1d2ee74d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3535137f-e40c-41fc-8c69-4287287f1a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41513941-839c-4dc1-964a-dbefda4224d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c61c6fd-25bc-4017-8aac-62f11ebd5701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c86706-fb76-4934-adec-4b3778b093be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996431e8-c8c4-4261-8115-8bba96a3d069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
